🎯 Multimodal Emotion Recognition（Image + Text）

多模态情绪识别 · 产品化能力组件（个人独立项目）
---
一、项目背景
---
随着社交媒体中图文内容的增长，单模态情绪识别无法准确捕捉真实语境。本项目由本人独立完成，从需求拆解、数据标注、模型设计到产品化落地，最终将多模态情绪识别封装为 Coze 智能体节点，方便在任意工作流中复用。

二、项目目标（What）
---
构建一套可落地、可复用的多模态情绪识别基础能力，输入图像与文本，输出五类情绪标签及概率分布。

输入：图像 + 文本

输出：情绪标签（积极 / 消极 / 愤怒 / 平静 / 中性）

场景：可在 Coze 流程节点中直接调用

三、个人贡献（Solo Project Highlights）
---
本项目所有内容（数据标注、建模、开发、部署、文档）均由本人独立完成，包括：

1. 产品设计

负责整体需求拆解、情绪标签体系设计

设计输入输出规范、异常处理规则

制定评估指标体系（Accuracy / F1 / Error Case 分析）

2. 技术方案设计与实现

基线模型：CLIP 图文特征 + DistilBERT 情绪特征

自研模块：FusionHead 多模态融合层

训练、调参与模型性能验证均由本人独立完成

3. 数据获取与标注

调研可用图文数据集

本地构建数据清洗脚本

自开发 GUI 标注工具，手动标注 1500+ 条图文情绪数据

4. 产品化与部署

将模型封装为 Coze 能力节点

设计标准化接口输出格式，支持可视化工作流

完成错误提示、置信度阈值和回退策略逻辑

四、系统架构（Architecture）
---
图像 + 文本
     → CLIP 特征提取（512维）
     → DistilBERT 文本情绪特征
     → FusionHead（自定义融合层）
     → 输出五类情绪概率

五、关键成果（Key Results）
---
多模态融合模型相较加权基线 F1 提升约 8–12%

1500 条自建高一致性情绪标注数据集

成功部署为 Coze 智能体节点：可直接拖拽使用

提供结构化 JSON 输出，便于上层业务调用

项目从 0–1 全流程均由本人独立实现

六、项目价值（Why）
---
1. 让情绪识别成为“能力组件”

业务方无需懂模型，只需在 Coze 中拖拽节点即可获得：

图文情绪判断

情绪向量输出

情绪变化监测

2. 深度服务多场景

智能客服：情绪化回复

内容推荐：将情绪作为特征

舆情洞察：监测趋势变化

AI 助手：更自然的人机互动

3. 多模态提升真实性

能识别情绪冲突（文案开心但照片悲伤）、反讽等复杂语境。

七、使用示例（Coze 节点调用）
---
{
  "input_image": "path/to/img.jpg",
  "input_text": "I really don't know how to feel today...",
  "output": {
    "label": "sad",
    "probabilities": {
      "happy": 0.12,
      "neutral": 0.18,
      "sad": 0.62,
      "angry": 0.05,
      "calm": 0.03
    }
  }
}

八、未来规划（Next Steps）
---

扩展更多细粒度情绪：期待、焦虑、孤独、厌恶

加入音频模态，构建“三模态情绪识别”

开发更完善的情绪动态监测 API

在 Coze 上封装情绪驱动型对话策略（识别 → 生成回应）
