import os
import re
import torch
import torch.nn.functional as F
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
from textwrap import fill
from transformers import (
    CLIPProcessor, CLIPModel,
    AutoTokenizer, AutoModelForSequenceClassification,
    MarianMTModel, MarianTokenizer
)
from models.fusion_head import FusionHead
from models.clip_encoder import CLIPEncoder
from utils.label_map import map_label_soft

# âœ… é˜²æ­¢æœåŠ¡å™¨æ— æ˜¾ç¤ºæ—¶æŠ¥é”™
matplotlib.use('Agg')
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False


# -----------------------------------------------------
# ğŸ” æ£€æµ‹è¯­è¨€
# -----------------------------------------------------
def detect_language(text: str) -> str:
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(text)
    ratio = chinese_chars / total_chars if total_chars > 0 else 0
    return "chinese" if ratio > 0.3 else "english"


# -----------------------------------------------------
# ğŸŒ ä¸­æ–‡ç¿»è¯‘ï¼ˆä¼˜å…ˆç”¨ opus-mt-zh-en å®Œæ•´ç¿»è¯‘ï¼‰
# -----------------------------------------------------
def translate_chinese_to_english(text):
    model_path = "/home/data/xiaoyu/models/opus-mt-zh-en"

    if os.path.exists(model_path):
        try:
            print("ğŸŒ æ£€æµ‹åˆ°æœ¬åœ°ç¿»è¯‘æ¨¡å‹ï¼Œä½¿ç”¨ opus-mt-zh-en å®Œæ•´ç¿»è¯‘...")
            tokenizer = MarianTokenizer.from_pretrained(model_path, local_files_only=True)
            model = MarianMTModel.from_pretrained(model_path, local_files_only=True)
            inputs = tokenizer(text, return_tensors="pt", truncation=True)
            translated = model.generate(**inputs, max_length=256)
            result = tokenizer.decode(translated[0], skip_special_tokens=True)
            return result
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯æ›¿æ¢æ¨¡å¼ã€‚åŸå› : {e}")

    # ğŸš‘ å›é€€æ¨¡å¼ï¼šå…³é”®è¯æ›¿æ¢
    translations = {
        'å¿«ä¹': 'happy', 'é«˜å…´': 'happy', 'å¼€å¿ƒ': 'happy',
        'æ‚²ä¼¤': 'sad', 'éš¾è¿‡': 'sad', 'ä¼¤å¿ƒ': 'sad',
        'ç”Ÿæ°”': 'angry', 'æ„¤æ€’': 'angry', 'æ¼ç«': 'angry',
        'å¹³é™': 'calm', 'å®‰å®': 'calm', 'å¹³å’Œ': 'calm',
        'ç„¦è™‘': 'anxious', 'ç´§å¼ ': 'anxious', 'æ‹…å¿ƒ': 'worried',
        'æ¼‚äº®': 'beautiful', 'ç¾ä¸½': 'beautiful',
        'è°¢è°¢': 'thank you', 'æ„Ÿè°¢': 'thanks',
        'å¿ƒæƒ…': 'mood', 'æƒ…ç»ª': 'emotion',
        'æ²»æ„ˆ': 'healing', 'å®‰æ…°': 'comfort'
    }
    translated = text
    for cn, en in translations.items():
        translated = translated.replace(cn, en)
    return translated


# -----------------------------------------------------
# ğŸ§  ä¸»é¢„æµ‹å‡½æ•°
# -----------------------------------------------------
def predict(image_path, text, device='cpu', use_trainable_fusion=True,
            fusion_checkpoint='/home/data/xiaoyu/CLIPMulti_reduced_project/checkpoints/fusion_head.pt'):
    device = torch.device(device)
    print(f"ğŸ–¥ï¸ å½“å‰è¿è¡Œè®¾å¤‡ï¼š{device}")

    # âœ… ä½¿ç”¨æœ¬åœ° CLIP æ¨¡å‹
    clip_model_path = '/home/data/xiaoyu/models/clip-vit-base-patch16'
    clip_model = CLIPModel.from_pretrained(clip_model_path, local_files_only=True).to(device)
    clip_processor = CLIPProcessor.from_pretrained(clip_model_path, local_files_only=True)

    # âœ… æ£€æµ‹è¯­è¨€å¹¶ç¿»è¯‘
    lang = detect_language(text)
    print(f"ğŸŒ æ£€æµ‹åˆ°è¾“å…¥è¯­è¨€ï¼š{lang}")
    original_text = text
    clip_text = translate_chinese_to_english(text) if lang == "chinese" else text
    if lang == "chinese":
        print(f"ğŸ”¤ ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡: {text} -> {clip_text}")

    # âœ… æœ¬åœ°è‹±æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆæ‰‹åŠ¨æ¨ç†ï¼‰
    print("âœ… ä½¿ç”¨æœ¬åœ°è‹±æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆæ‰‹åŠ¨æ¨ç†ï¼‰")
    sentiment_model_path = "/home/data/xiaoyu/models/textattack-distilbert-base-uncased-imdb"
    tokenizer = AutoTokenizer.from_pretrained(sentiment_model_path, local_files_only=True)
    sent_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path, local_files_only=True).to(device)
    sent_model.eval()

    inputs_for_sent = tokenizer(clip_text, return_tensors='pt', truncation=True, padding=True)
    inputs_for_sent = {k: v.to(device) for k, v in inputs_for_sent.items()}

    with torch.no_grad():
        outputs_sent = sent_model(**inputs_for_sent)
        probs_sent = F.softmax(outputs_sent.logits, dim=-1).cpu().squeeze(0)

    if hasattr(sent_model.config, 'id2label') and isinstance(sent_model.config.id2label, dict):
        id2label = sent_model.config.id2label
    else:
        id2label = {0: 'NEGATIVE', 1: 'POSITIVE'}

    label_id = int(torch.argmax(probs_sent).item())
    text_label = id2label.get(label_id, str(label_id))
    score = float(probs_sent[label_id].item())
    text_result_raw = {'label': text_label, 'score': score}

    # âœ… æ–‡æœ¬ soft åˆ†å¸ƒ
    soft_dist = map_label_soft(text_label)
    text_pseudo = torch.zeros(5)
    for k, v in soft_dist.items():
        text_pseudo[k] = v * score
    text_pseudo = text_pseudo / text_pseudo.sum()

    # âœ… å›¾åƒç‰¹å¾é¢„æµ‹
    image = Image.open(image_path).convert('RGB')
    candidate_texts = [
        'a happy bright sunny scene',
        'a sad gloomy scene',
        'an angry violent scene',
        'a calm peaceful scene',
        'an anxious tense scene'
    ]
    inputs = clip_processor(text=candidate_texts, images=image, return_tensors='pt', padding=True).to(device)

    with torch.no_grad():
        outputs = clip_model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = F.softmax(logits_per_image, dim=1).cpu().squeeze(0)

    # âœ… èåˆå±‚
    if use_trainable_fusion and os.path.exists(fusion_checkpoint):
        print(f"ğŸ§  ä½¿ç”¨è®­ç»ƒå¥½çš„èåˆå±‚ï¼š{fusion_checkpoint}")
        clip_encoder = CLIPEncoder(model_name=clip_model_path, device=device)
        fusion = FusionHead(img_dim=512, txt_dim=512, hidden=512, num_classes=5).to(device)
        fusion.load_state_dict(torch.load(fusion_checkpoint, map_location=device))
        fusion.eval()

        # âœ… æŠŠ PIL å›¾åƒå…ˆè½¬ä¸º CLIP é¢„å¤„ç†åçš„ tensor
        # âœ… ç›´æ¥ä¼ å…¥åŸå§‹ PIL å›¾åƒï¼Œè®© clip_encoder è‡ªå·±å¤„ç†
        img_feat = clip_encoder.encode_image(image).to(device)
        txt_feat = clip_encoder.encode_text([clip_text]).to(device)

        txt_feat = clip_encoder.encode_text([clip_text]).to(device)

        with torch.no_grad():
            logits = fusion(img_feat, txt_feat)
            combined = F.softmax(logits, dim=1).cpu().squeeze(0)
        method = 'trained_fusion_head'
    else:
        print("âš™ï¸ æœªæ£€æµ‹åˆ°èåˆå±‚æˆ–æœªå¯ç”¨è®­ç»ƒèåˆï¼Œä½¿ç”¨è§„åˆ™èåˆã€‚")
        combined = 0.6 * probs + 0.4 * text_pseudo
        combined = combined / combined.sum()
        method = 'rule_based_softmap'

    # âœ… è¾“å‡ºç»“æœ
    comb_top = int(torch.argmax(combined).item())
    fused_result = {
        'method': method,
        'scores': {t: float(s) for t, s in zip(candidate_texts, combined.tolist())},
        'top': candidate_texts[comb_top],
        'original_text': original_text,
        'translated_text': clip_text if lang == "chinese" else None
    }

    visualize_results(image, original_text, fused_result)
    return fused_result

# -----------------------------------------------------
# ğŸ“Š å¯è§†åŒ–ï¼ˆä»…è‹±æ–‡ï¼Œä¸æ˜¾ç¤ºä¸­æ–‡ï¼‰
# -----------------------------------------------------
def visualize_results(image, input_text, fused_result):
    scores = fused_result["scores"]
    labels = list(scores.keys())
    values = list(scores.values())
    top_label = fused_result["top"]

    plt.figure(figsize=(12, 6))

    # ---- å·¦ä¾§æ˜¾ç¤ºå›¾ç‰‡ + è‹±æ–‡æ–‡æœ¬ ----
    plt.subplot(1, 2, 1)
    plt.imshow(image)
    plt.axis("off")

    # å¦‚æœæœ‰ç¿»è¯‘ï¼Œå°±æ˜¾ç¤ºç¿»è¯‘ï¼›å¦åˆ™æ˜¾ç¤ºåŸæ–‡æœ¬
    caption = fused_result.get("translated_text") or fused_result["original_text"]
    # ç”¨è‡ªåŠ¨æ¢è¡Œå¤„ç†é•¿æ–‡æœ¬
    caption_wrapped = fill(caption, width=40)
    plt.text(0.5, 1.05, caption_wrapped,
             transform=plt.gca().transAxes,
             fontsize=12, ha='center', va='bottom',
             wrap=True, style='italic', color='#333')

    # ---- å³ä¾§æ˜¾ç¤ºé¢„æµ‹æŸ±çŠ¶å›¾ ----
    plt.subplot(1, 2, 2)
    bars = plt.barh(labels, values)
    for bar, label in zip(bars, labels):
        if label == top_label:
            bar.set_color('orange')
    plt.xlabel("Probability", fontsize=10)
    plt.title(f'Predicted Emotion: {top_label}', fontsize=12)
    plt.tight_layout()
    plt.savefig("result_visual.png", bbox_inches='tight')
    print("âœ… Visualization saved as result_visual.png (English only)")


# -----------------------------------------------------
# ğŸ CLI å…¥å£
# -----------------------------------------------------
def predict_from_args(args):
    predict(
        image_path=args.image_path,
        text=args.text,
        device=args.device,
        use_trainable_fusion=True,
        fusion_checkpoint='/home/data/xiaoyu/CLIPMulti_reduced_project/CLIPMulti_reduced/checkpoints/fusion_head.pt'
    )
