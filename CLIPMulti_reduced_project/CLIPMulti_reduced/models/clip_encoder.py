from transformers import CLIPModel, CLIPProcessor
import torch


class CLIPEncoder:
    def __init__(self, model_name, device="cuda"):
        print(f"ğŸ§  Loading CLIP model from {model_name} to {device}")

        # ç¡®å®šè®¾å¤‡
        if device == "cuda" and torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")

        # åœ¨ç›®æ ‡è®¾å¤‡ä¸ŠåŠ è½½æ¨¡å‹
        self.model = CLIPModel.from_pretrained(
            model_name,
            local_files_only=True
        ).to(self.device)

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        self.processor = CLIPProcessor.from_pretrained(model_name, local_files_only=True)
        print(f"âœ… CLIPæ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")

    def encode_image(self, images):
        # è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç±»å‹
        if isinstance(images, torch.Tensor):
            # å·²ç»æ˜¯ tensorï¼Œç›´æ¥é€å…¥æ¨¡å‹ï¼Œä¸å†äºŒæ¬¡é¢„å¤„ç†
            with torch.no_grad():
                image_features = self.model.get_image_features(images)
            return image_features
        else:
            # å¦åˆ™å‡è®¾æ˜¯ PIL å›¾åƒæˆ–è·¯å¾„ï¼Œè¿›è¡Œæ ‡å‡†é¢„å¤„ç†
            inputs = self.processor(images=images, return_tensors="pt").to(self.device)
            with torch.no_grad():
                image_features = self.model.get_image_features(**inputs)
            return image_features

    def encode_text(self, texts):
        """ç¼–ç æ–‡æœ¬"""
        if isinstance(texts, str):
            texts = [texts]

        inputs = self.processor(
            text=texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=77
        ).to(self.device)

        with torch.no_grad():
            txt_features = self.model.get_text_features(**inputs)
        return txt_features